[TOC]

# Implement detail

* Q: Implementation of Discretization B [issue 114](https://github.com/state-spaces/mamba/issues/114) [issue10](https://github.com/alxndrTL/mamba.py/issues/10)

  * ä»£ç ä¸­æ²¡æœ‰ç”¨$\bar{B_t} = (âˆ†A)^{-1}(exp(âˆ†A)âˆ’I)\cdot âˆ†B$ è€Œæ˜¯ç”¨äº†ç®€åŒ–ç‰ˆçš„ $âˆ†B$ (æ²¡æœ‰ç”¨åŽŸæ–‡ä¸­æ‰¯åˆ°çš„ZOH)

    > It makes the implementation slightly simpler without affecting empirical performance. One can think of it as a mix of ZOH for A and Euler discretization for B. 

* Throughput related
  * cg=True matters [issue90](https://github.com/state-spaces/mamba/issues/90), to reduce CPU/IO

* Parameters

  ![image-20240222171235795](/Users/kakusou/PaperComments/Images/image-20240222171235795.png)

  * ä¸€ä¸ªmamba blockçš„å‚æ•°é‡ (D: model_size, N: dstate, e: expand_factor, K: conv1d_kernel_size)

    * $2eD^2 + eD(K+1) + eD(D/16+2N) + eD(D/16+1) +  eDN + eD + eD^2$
    * å–e=2,  æ€»å‚æ•°é‡ï¼š$6.25D^2 + 5DN + 4D + 2D(K+1)$
    * å–N=16, K=4, æ€»å‚æ•°é‡: $6.25D^2+94D$
    * In detail
  
      * Conv block: 
  
        $\#ð‘ð‘Žð‘Ÿð‘Žð‘šð‘ =(ð‘˜ð‘’ð‘Ÿð‘›ð‘’ð‘™\_ð‘ ð‘–ð‘§ð‘’ \times |\frac{in\_channel}{groups}| + 1)Ã—|ð‘œð‘¢ð‘¡\_ð‘â„Žð‘Žð‘›ð‘›ð‘’ð‘™ð‘ |$
  
  * åŒå‘mamba
  
    ![image-20240222171305761](/Users/kakusou/PaperComments/Images/image-20240222171305761.png)
  
    * $6D^2 + 2(2D(K+1) + 0.25D^2 + 5DN + 4D)$
    * å–N=16, K=4, æ€»å‚æ•°é‡: $6.5D^2+188D$
  
  * For reference: each transformer layer: $12D^2$

# Train **detail**

1. language modeling - scaling law (è¡¥å……ä¿¡æ¯æ¥è‡ª[issue144](https://github.com/state-spaces/mamba/issues/144))

   1. ![image-20240221214830835](/Users/kakusou/PaperComments/Images/image-20240221214830835.png)æ³¨æ„è¿™é‡Œçš„lræ˜¯baseï¼Œå®žé™…ä¼šx5

      * 2.8B model lr = 3xGPT3(1.6x1e-4) = 4.8x1e-4

   2. åŽŸæ–‡Train recipesä¸­å†™é“

      > gradient clip = 1.0; 
      > weight decay = 0.1; 
      > no dropout; 
      > linear lr warmup + cosine decay (decay to 1e-5, peak value set $5\times$ GPT3 value)
      >
      > * å‚è€ƒGPT-3çš„scalingè®¾å®š
      >   * <img src="/Users/kakusou/PaperComments/Images/image-20240221215230005.png" alt="image-20240221215230005" style="zoom:67%;" /> 
      >
      > no bias term:
      > RMSNorm (instead of LayerNorm)
      > AdamW $\beta=(0.9, 0.95)$



# Install Micromamba

```bash
# py39 torch211+cu118: /cto_labas/AIDD/mamba/vllm.yaml

# 1. create new env [Name: prot]
1.9.0 prot python==3.9

# 2. install pytorch related
## pytorch version: 2.1.1+cu118
pip3 install torch==2.1.1+cu118 torchvision==0.16.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html

## pytorch verison: 1.13.1+cu116
pip3 install torch==1.13.1+cu116 torchvision==0.14.1+cu116 -f https://download.pytorch.org/whl/torch_stable.html
```
